{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import wandb\n",
    "import matplotlib.pyplot as plt\n",
    "from keras.datasets import fashion_mnist\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix , accuracy_score\n",
    "from scipy.special import log_softmax,softmax\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FFNN:\n",
    "    def __init__(self,net_size,layer_act,init_wb='he',lr=1e-3,opt='nadam',lamda=0,batch_size=64,\\\n",
    "                 n_epochs=10,beta_1=0.9,beta_2=0.999,seed=None,loss='cross_ent',relu_param=0):\n",
    "        \n",
    "        self.net_size = net_size\n",
    "        self.layer_acts = layer_act\n",
    "        self.init_wb = init_wb\n",
    "        self.lr = lr\n",
    "        self.optim = opt\n",
    "        self.lamda = lamda\n",
    "        self.batch_size = batch_size\n",
    "        self.n_epochs = n_epochs\n",
    "        self.beta_1 = beta_1\n",
    "        self.beta_2 = beta_2\n",
    "        self.loss = loss\n",
    "        self.seed = seed\n",
    "        self.relu_param=relu_param\n",
    "\n",
    "    def onehot_encode(self,y, n_labels):\n",
    "        mat = np.zeros((len(y), n_labels))\n",
    "        for i, val in enumerate(y):\n",
    "            mat[i, val] = 1\n",
    "        return mat.T\n",
    "    \n",
    "    def nn_init(self, network_size, wb_init='random'):\n",
    "\n",
    "        if self.seed is not None:\n",
    "            np.random.seed(self.seed)\n",
    "\n",
    "        params = {}\n",
    "\n",
    "        num_layers = len(network_size)\n",
    "\n",
    "        if wb_init == 'random':\n",
    "            for layer in range(1, num_layers):\n",
    "                params['weights' + str(layer)] = np.random.random((network_size[layer], network_size[layer - 1]))\n",
    "                params['biases' + str(layer)] = np.random.random((network_size[layer], 1))\n",
    "                \n",
    "        elif wb_init == 'xavier_uniform':\n",
    "            for layer in range(1, num_layers):\n",
    "                r = np.sqrt(6.0 / (network_size[layer] + network_size[layer - 1]))\n",
    "                params['weights' + str(layer)] = np.random.uniform(-r, r, (network_size[layer], network_size[layer - 1]))\n",
    "                params['biases' + str(layer)] = np.random.uniform(-r, r, (network_size[layer], 1))\n",
    "        \n",
    "        else:\n",
    "            raise ValueError('Invalid Activation function ...')\n",
    "        return params\n",
    "    \n",
    "    def Linear(self,input_data,diff=False):\n",
    "        input_data = np.array(input_data, dtype=np.float64)\n",
    "        if diff == False:\n",
    "            return input_data\n",
    "        else:\n",
    "            return np.ones_like(input_data)\n",
    "\n",
    "    def ReLU(self, input_data, diff=False):\n",
    "        alpha = self.relu_param\n",
    "        input_data = np.array(input_data, dtype=np.float64)\n",
    "\n",
    "        if diff == False:\n",
    "            return np.where(input_data < 0, alpha * input_data, input_data)\n",
    "\n",
    "        elif diff == True:\n",
    "            output_data = np.ones_like(input_data, dtype=np.float64)\n",
    "            output_data[input_data < 0] = alpha\n",
    "            return output_data\n",
    "    \n",
    "    def ELU(self,input_data,diff=False):\n",
    "        alpha = self.relu_param\n",
    "        if diff == False:\n",
    "            return np.where(input_data < 0, alpha * (np.exp(input_data)-1), input_data)\n",
    "        else:\n",
    "            output_data = np.ones_like(input_data, dtype=np.float64)\n",
    "            output_data[input_data < 0] = alpha*np.exp(input_data)\n",
    "            return output_data    \n",
    "    \n",
    "    def sigmoid(self, input_data, diff=False):\n",
    "        input_data = np.where(input_data<-700,-700,input_data)\n",
    "        if not diff:\n",
    "            output_data = 1 / (1 + np.exp(-np.array(input_data)))\n",
    "        else:\n",
    "            s = 1 / (1 + np.exp(-np.array(input_data)))\n",
    "            output_data = s * (1 - s)\n",
    "        return output_data\n",
    "\n",
    "    def Tanh(self, input_data, diff=False):\n",
    "        # Compute the hyperbolic tangent function for input_data\n",
    "        input_data = np.array(input_data)\n",
    "        if not diff:\n",
    "            output_data = np.tanh(input_data)\n",
    "        # Compute the derivative of the hyperbolic tangent function for input_data\n",
    "        else:\n",
    "            output_data = 1 - np.tanh(input_data) ** 2\n",
    "        return output_data\n",
    "    \n",
    "    def softmax(self,X):\n",
    "        return log_softmax(X,axis=0)\n",
    "\n",
    "    def forward(self,data,acts,params):\n",
    "        if self.seed is not None:\n",
    "            np.random.seed(self.seed)\n",
    "        param_list = []\n",
    "        act_out = data\n",
    "        for idx, act in enumerate(acts,start=1):\n",
    "            data_prev = act_out\n",
    "            Wb = np.dot(params['weights'+str(idx)],data_prev)+params['biases'+str(idx)]\n",
    "\n",
    "            if act == 'sigmoid':\n",
    "                act_out = self.sigmoid(Wb)\n",
    "            elif act == 'tanh':\n",
    "                act_out = self.Tanh(Wb)\n",
    "            elif act == 'relu':\n",
    "                act_out = self.ReLU(Wb)\n",
    "            elif act == 'softmax':\n",
    "                act_out = self.softmax(Wb)\n",
    "            elif act == 'identity':\n",
    "                act_out = self.Linear(Wb)\n",
    "            elif act == 'elu':\n",
    "                act_out == self.ELU(Wb)\n",
    "            else:\n",
    "                raise ValueError('Invalid activation function ...')\n",
    "            \n",
    "            pl = ((data_prev,params['weights'+str(idx)],params['biases'+str(idx)]),Wb)\n",
    "            param_list.append(pl)\n",
    "        return act_out,param_list\n",
    "    \n",
    "    def grad(self,pred,target,params,lamda=0,loss='cross_ent'):\n",
    "        n_class = target.shape[1]\n",
    "        if loss == 'cross_ent':\n",
    "            loss = -np.mean(np.multiply(pred,target),axis=1).sum()\n",
    "        elif loss=='mse':\n",
    "            loss = -np.mean(np.multiply(pred-target,pred-target),axis=1).sum()\n",
    "        else:\n",
    "            raise ValueError('Error function invalid. Please choose either \"cross_ent\" or \"mse\" ')\n",
    "        param_len = len(params)//2\n",
    "\n",
    "        sum_w = 0\n",
    "        for idx in range(1,param_len):\n",
    "            sum_w += np.square(params['weights'+str(idx)]).sum()\n",
    "        loss += sum_w*(lamda/(2*n_class))\n",
    "        return loss\n",
    "\n",
    "    def backward(self,pred,target,param_list,acts,lamda=0,loss='cross_ent'):\n",
    "        grad_tape = {}\n",
    "        lpl = len(param_list)\n",
    "        m,n = pred.shape\n",
    "        target = target.reshape(pred.shape)\n",
    "        if loss == 'cross_ent':\n",
    "            dOut = np.exp(pred) - target\n",
    "        elif loss == 'mse':\n",
    "            dOut = 2*(np.exp(pred) - target)\n",
    "        else:\n",
    "            raise ValueError('Error function invalid. Please choose either \"cross_ent\" or \"mse\" ')\n",
    "\n",
    "        pred,weight,_ = param_list[-1][0]\n",
    "        grad_tape['d_weights'+str(lpl)] = np.dot(dOut,pred.T)/m\n",
    "        grad_tape['d_biases'+str(lpl)] = dOut.sum(axis=1,keepdims=True)/m\n",
    "        grad_tape['d_pred'+str(lpl-1)] = np.dot(weight.T,dOut)\n",
    "\n",
    "        for idx in reversed(range(lpl-1)):\n",
    "            linear_pred,out = param_list[idx]\n",
    "            out_prev,weight,b = linear_pred\n",
    "\n",
    "            m,n = out_prev.shape\n",
    "            dOut_prev = grad_tape['d_pred'+str(idx+1)]\n",
    "\n",
    "            if acts[idx] == 'relu':\n",
    "                dOut = dOut_prev*self.ReLU(out,True)\n",
    "            elif acts[idx] == 'sigmoid':\n",
    "                dOut = dOut_prev*self.sigmoid(out,True)\n",
    "            elif acts[idx] == 'tanh':\n",
    "                dOut = dOut_prev*self.Tanh(out,True)\n",
    "            elif acts[idx] == 'elu':\n",
    "                dOut = dOut_prev*self.ELU(out,True)\n",
    "            elif acts[idx] == 'identity':\n",
    "                dOut = dOut_prev*self.Linear(out,True)            \n",
    "            \n",
    "            grad_tape['d_pred'+str(idx)] = np.dot(weight.T,dOut)\n",
    "            grad_tape['d_weights'+str(idx+1)] = (np.dot(dOut,out_prev.T)+ lamda*weight)/m\n",
    "            grad_tape['d_biases'+str(idx+1)] = dOut.sum(axis=1,keepdims=True)/m\n",
    "        return grad_tape\n",
    "        \n",
    "\n",
    "    def optim_step(self,params,grad_tape,lr,t_step,algo='adam',opt_params=None):\n",
    "        len_param = len(params)//2\n",
    "\n",
    "        if algo == 'sgd':\n",
    "            for idx in range(len_param):\n",
    "                params['weights'+str(idx+1)] -= lr*grad_tape['d_weights'+str(idx+1)]\n",
    "                params['biases'+str(idx+1)] -= lr*grad_tape['d_biases'+str(idx+1)]\n",
    "                opt_params=None\n",
    "        elif algo == 'sgdm':\n",
    "            for idx in range(len_param):\n",
    "                opt_params['v_w'+str(idx+1)] = self.beta_1*opt_params['v_w'+str(idx+1)] + (1-self.beta_1)*grad_tape['d_weights'+str(idx+1)]\n",
    "                opt_params['v_w'+str(idx+1)] = self.beta_1*opt_params['v_w'+str(idx+1)] + (1-self.beta_1)*grad_tape['d_biases'+str(idx+1)]\n",
    "\n",
    "                params['weights'+str(idx+1)] -= lr*opt_params['v_w'+str(idx+1)]\n",
    "                params['biases'+str(idx+1)] -= lr*opt_params['v_w'+str(idx+1)]\n",
    "        elif algo == 'nag':\n",
    "            for idx in range(len_param):\n",
    "                opt_params['v_w'+str(idx+1)] = self.beta_1*opt_params['v_w'+str(idx+1)] - lr*grad_tape['d_weights'+str(idx+1)]\n",
    "                opt_params['vb'+str(idx+1)] = self.beta_1*opt_params['v_w'+str(idx+1)] - lr*grad_tape['d_biases'+str(idx+1)]\n",
    "\n",
    "                params['weights'+str(idx+1)] -= self.beta_1*(opt_params['v_w'+str(idx+1)] - opt_params['v_w_prev'+str(idx+1)])\n",
    "                params['biases'+str(idx+1)] -= self.beta_1*(opt_params['v_w'+str(idx+1)] - opt_params['v_w_prev'+str(idx+1)])\n",
    "\n",
    "                opt_params['v_w_prev'+str(idx+1)] = opt_params['v_w'+str(idx+1)]\n",
    "                opt_params['v_w_prev'+str(idx+1)] = opt_params['v_w'+str(idx+1)]\n",
    "\n",
    "        elif algo == 'rmsprop':\n",
    "            for idx in range(len_param):\n",
    "                opt_params['m_b'+str(idx+1)] = self.beta_2*opt_params['m_b'+str(idx+1)] + (1-self.beta_2)*(grad_tape['d_biases'+str(idx+1)]**2)\n",
    "                opt_params['m_w'+str(idx+1)] = self.beta_2*opt_params['m_w'+str(idx+1)] + (1-self.beta_2)*(grad_tape['d_weights'+str(idx+1)]**2)\n",
    "\n",
    "                params['weights'+str(idx+1)] -= lr*grad_tape['d_weights'+str(idx+1)]/(np.sqrt(opt_params['m_w'+str(idx+1)])+1e-8)\n",
    "                params['biases'+str(idx+1)] -= lr*grad_tape['d_biases'+str(idx+1)]/(np.sqrt(opt_params['m_b'+str(idx+1)])+1e-8)\n",
    "\n",
    "        elif algo == 'adam':\n",
    "            for idx in range(len_param):\n",
    "                opt_params['v_w'+str(idx+1)] = self.beta_1*opt_params['v_w'+str(idx+1)] + (1-self.beta_1)*grad_tape['d_biases'+str(idx+1)]\n",
    "                opt_params['v_w'+str(idx+1)] = self.beta_1*opt_params['v_w'+str(idx+1)] + (1-self.beta_1)*grad_tape['d_weights'+str(idx+1)]\n",
    "\n",
    "                opt_params['m_b'+str(idx+1)] = self.beta_2*opt_params['m_b'+str(idx+1)] + (1-self.beta_2)*(grad_tape['d_biases'+str(idx+1)]**2)\n",
    "                opt_params['m_w'+str(idx+1)] = self.beta_2*opt_params['m_w'+str(idx+1)] + (1-self.beta_2)*(grad_tape['d_weights'+str(idx+1)]**2)\n",
    "\n",
    "                mod_lr = lr*np.sqrt((1-self.beta_2**t_step)/(1-self.beta_1**t_step+1e-8))\n",
    "                params['weights'+str(idx+1)] -= mod_lr*(opt_params['v_w'+str(idx+1)]/(np.sqrt(opt_params['m_w'+str(idx+1)])+1e-8))\n",
    "                params['biases'+str(idx+1)] -= mod_lr*(opt_params['v_w'+str(idx+1)]/(np.sqrt(opt_params['m_b'+str(idx+1)])+1e-8))\n",
    "        elif algo =='nadam':\n",
    "            for idx in range(len_param):\n",
    "                opt_params['v_w'+str(idx+1)] = self.beta_1*opt_params['v_w'+str(idx+1)] + (1-self.beta_1)*grad_tape['d_biases'+str(idx+1)]\n",
    "                opt_params['v_w'+str(idx+1)] = self.beta_1*opt_params['v_w'+str(idx+1)] + (1-self.beta_1)*grad_tape['d_weights'+str(idx+1)]\n",
    "\n",
    "                opt_params['m_b'+str(idx+1)] = self.beta_2*opt_params['m_b'+str(idx+1)] + (1-self.beta_2)*(grad_tape['d_biases'+str(idx+1)]**2)\n",
    "                opt_params['m_w'+str(idx+1)] = self.beta_2*opt_params['m_w'+str(idx+1)] + (1-self.beta_2)*(grad_tape['d_weights'+str(idx+1)]**2)\n",
    "\n",
    "                mod_lr = lr*np.sqrt((1-self.beta_2**t_step)/(1-self.beta_1**t_step+1e-8))\n",
    "                params['weights'+str(idx+1)] -= (mod_lr/(np.sqrt(opt_params['m_w'+str(idx+1)])+1e-8))*(self.beta_1*opt_params['v_w'+str(idx+1)] + (1-self.beta_1)*grad_tape['d_weights'+str(idx+1)])\n",
    "                params['biases'+str(idx+1)] -= (mod_lr/(np.sqrt(opt_params['m_b'+str(idx+1)])+1e-8))*(self.beta_1*opt_params['v_w'+str(idx+1)] + (1-self.beta_1)*grad_tape['d_biases'+str(idx+1)])\n",
    "        return params,opt_params\n",
    "    \n",
    "    def predict(self,data):\n",
    "        out = self.forward(data,self.layer_acts,self.params)[0]\n",
    "        return np.argmax(out,axis=0),out.T\n",
    "    \n",
    "    def train(self,X_train,Y_train,X_val,Y_val,n_classes=10,wb_log=True):\n",
    "        self.losses=[]\n",
    "        opt_params = {}\n",
    "        m = X_train.shape[1]\n",
    "        y_train = self.onehot_encode(Y_train,n_classes)\n",
    "        self.params = self.nn_init(self.net_size,self.init_wb)\n",
    "        self.t_step = 1\n",
    "        idx = np.arange(m)\n",
    "\n",
    "        if self.optim != 'sgd':\n",
    "            for ii in range(1,len(self.net_size)):\n",
    "                opt_params['v_w'+str(ii)] = np.zeros((self.net_size[ii],self.net_size[ii-1]))\n",
    "                opt_params['v_w'+str(ii)] = np.zeros((self.net_size[ii],1))\n",
    "\n",
    "                opt_params['v_w_prev'+str(ii)] = np.zeros((self.net_size[ii],self.net_size[ii-1]))\n",
    "                opt_params['v_w_prev'+str(ii)] = np.zeros((self.net_size[ii],1))\n",
    "\n",
    "                opt_params['m_w'+str(ii)] = np.zeros((self.net_size[ii],self.net_size[ii-1]))\n",
    "                opt_params['m_b'+str(ii)] = np.zeros((self.net_size[ii],1))\n",
    "        \n",
    "        for _ in range(self.n_epochs):\n",
    "            np.random.shuffle(idx)\n",
    "            X_shuffled = X_train[:,idx]\n",
    "            Y_shuffled = y_train[:,idx]\n",
    "            for ii in range(0,m,self.batch_size):\n",
    "                # self.t_step+=1\n",
    "                X_batched = X_shuffled[:,ii:ii+self.batch_size]\n",
    "                Y_batched = Y_shuffled[:,ii:ii+self.batch_size]\n",
    "\n",
    "                out,param_list = self.forward(X_batched,self.layer_acts,self.params)\n",
    "                loss = self.grad(out,Y_batched,self.params,self.lamda,self.loss)\n",
    "                self.losses.append(loss)\n",
    "                grads = self.backward(out,Y_batched,param_list,self.layer_acts,self.lamda,self.loss)\n",
    "                self.params,opt_params = self.optim_step(self.params,grads,self.lr,\\\n",
    "                                                            self.t_step,self.optim,opt_params)\n",
    "                self.t_step+=1\n",
    "            y_pred_train,_ = self.predict(X_train)\n",
    "            y_pred_valid,_ = self.predict(X_val)\n",
    "\n",
    "            train_acc = accuracy_score(Y_train,y_pred_train)\n",
    "            val_acc = accuracy_score(Y_val,y_pred_valid)\n",
    "            log = {'train_acc':train_acc, 'val_acc':val_acc,'train_loss':loss}\n",
    "            if wb_log:\n",
    "                wandb.log(log)\n",
    "            else:\n",
    "                print(log)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_names = ['T-shirt/top', 'Trouser', 'Pullover', 'Dress',\n",
    "               'Coat', 'Sandal', 'Shirt', 'Sneaker', 'Bag', 'Ankle boot']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(x_train, y_train), (x_test, y_test) = fashion_mnist.load_data()\n",
    "\n",
    "class_num = 10\n",
    "num_row = 2\n",
    "num_col = 5# plot images\n",
    "fig, axes = plt.subplots(num_row, num_col, figsize=(1.5*num_col,2*num_row))\n",
    "img_list=[]\n",
    "for i in range(class_num):\n",
    "  ax = axes[i//num_col, i%num_col]\n",
    "  a = np.argmax(y_train == i)\n",
    "  ax.imshow(x_train[a], cmap='gray')\n",
    "  ax.set_title(class_names[i])\n",
    "plt.tight_layout()\n",
    "\n",
    "x_train, x_valid, y_train, y_valid = train_test_split(x_train, y_train, test_size=0.1, random_state=0, stratify=y_train)\n",
    "\n",
    "x_train = x_train.reshape((len(x_train), 28*28))\n",
    "x_train = x_train.astype('float32') / 255\n",
    "\n",
    "x_valid = x_valid.reshape((len(x_valid), 28*28))\n",
    "x_valid = x_valid.astype('float32') / 255\n",
    "\n",
    "# Preprocessing test data\n",
    "x_test = x_test.reshape((len(x_test), 28 * 28))\n",
    "x_test = x_test.astype('float32') / 225"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X = x_train.T\n",
    "# X_valid = x_valid.T\n",
    "# Y_valid = y_valid\n",
    "# Y = y_train\n",
    "# n_class= 10\n",
    "# n_hidden = 3\n",
    "# layers = []\n",
    "# hidden_size=32\n",
    "# for i in range(n_hidden+2):\n",
    "#     if i == 0:\n",
    "#         layers.append(X.shape[0])\n",
    "#     elif i == n_hidden+1:\n",
    "#         layers.append(n_class)\n",
    "#     else:\n",
    "#         layers.append(hidden_size)\n",
    "#     i = i+1\n",
    "# act = []\n",
    "# o_act = 'softmax'\n",
    "# actvn_fn = 'tanh'\n",
    "# for i in range(n_hidden+1):\n",
    "#     if i == n_hidden:\n",
    "#         act.append(o_act)\n",
    "#     else:\n",
    "#         act.append(actvn_fn)\n",
    "#     i = i+1\n",
    "# model = FFNN(layers,act)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.train(X,Y,X_valid,Y_valid,wb_log=False)\n",
    "# model.params = model.nn_init(layers)\n",
    "# y_pred,_ = model.predict(x_test.T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sweep_config = {\n",
    "    'method':'bayes',\n",
    "    'metric':{\n",
    "    'name':'val_acc',\n",
    "    'goal':'maximize'\n",
    "    },\n",
    "    'parameters':{\n",
    "    'n_epochs':{\n",
    "    'values':[5,10]\n",
    "    },\n",
    "    'n_hidden':{\n",
    "    'values':[3,4,5]\n",
    "    },\n",
    "    'n_hidden_units':{\n",
    "    'values':[32,64,128]\n",
    "    },\n",
    "    'l2_coeff':{\n",
    "    'values':[0,5e-4,5e-1]\n",
    "    },\n",
    "    'lr':{\n",
    "    'values':[1e-3,1e-4]\n",
    "    },\n",
    "    'optim_algo':{\n",
    "    'values':['sgd','sgdm','rmsprop','adam','nadam','nag']\n",
    "    },\n",
    "    'batch_size':{\n",
    "    'values':[16,32,64]\n",
    "    },\n",
    "    'weight_init':{\n",
    "    'values':['random','xavier_uniform']\n",
    "    },\n",
    "    'act_func':{\n",
    "    'values':['relu','sigmoid','tanh','elu','identity']\n",
    "    },\n",
    "    'loss_func':{\n",
    "    'values':['cross_ent','mse']\n",
    "    },\n",
    "    'relu_param':{\n",
    "    'values':[0,1e-1,1e-2,1e-3]\n",
    "    }\n",
    "    }\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sweep_id = wandb.sweep(sweep_config, entity=\"viswa_ee\", project=\"CS6910\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def learn():\n",
    "    config_defaults={\n",
    "        'n_epochs':10,\n",
    "        'n_hidden':3,\n",
    "        'n_hidden_units':10,\n",
    "        'l2_coeff':0,\n",
    "        'lr':1e-3,\n",
    "        'optim_algo':'sgd',\n",
    "        'batch_size':16,\n",
    "        'weights_init':'random',\n",
    "        'act_func':'relu',\n",
    "        'loss_func':'cross_ent',\n",
    "        'relu_param':0\n",
    "    }\n",
    "    wandb.init(config=config_defaults)\n",
    "    config = wandb.config\n",
    "    X = x_train.T\n",
    "    X_valid = x_valid.T\n",
    "    Y_valid = y_valid\n",
    "    Y = y_train\n",
    "    n_class= 10\n",
    "    class_names = ['T-shirt/top', 'Trouser', 'Pullover', 'Dress',\n",
    "               'Coat', 'Sandal', 'Shirt', 'Sneaker', 'Bag', 'Ankle boot']\n",
    "\n",
    "    layers = []\n",
    "    for i in range(config.n_hidden+2):\n",
    "        if i == 0:\n",
    "            layers.append(X.shape[0])\n",
    "        elif i == config.n_hidden+1:\n",
    "            layers.append(n_class)\n",
    "        else:\n",
    "            layers.append(config.n_hidden_units)\n",
    "        i = i+1\n",
    "    \n",
    "    output_act = 'softmax'\n",
    "    act_fn = config.act_func\n",
    "\n",
    "    acts = []\n",
    "    for i in range(config.n_hidden+1):\n",
    "        if i == config.n_hidden:\n",
    "            acts.append(output_act)\n",
    "        else:\n",
    "            acts.append(act_fn)\n",
    "        i = i+1\n",
    "    \n",
    "    # wandb.run.name = str(config.optim_algo) +'_'+ str(config.act_func) + '_bs_' + str(config.batch_size)\n",
    "    model = FFNN(net_size=layers,layer_act=acts,init_wb=config.weights_init,lr=config.lr,opt=config.optim_algo,\\\n",
    "                 lamda=config.l2_coeff,batch_size=config.batch_size,n_epochs=config.n_epochs,loss=config.loss_func,relu_param=config.relu_param)\n",
    "    model.train(X,Y,X_valid,Y_valid)\n",
    "    y_test_pred,_ = model.predict(x_test.T)\n",
    "    cm = confusion_matrix(y_test, y_test_pred)\n",
    "\n",
    "    # # Define class names (optional)\n",
    "\n",
    "    # Normalize the confusion matrix (optional)\n",
    "    cm_norm = cm.astype(\"float\") / cm.sum(axis=1)[:, np.newaxis]\n",
    "\n",
    "    # Create a heatmap plot of the confusion matrix\n",
    "    fig, ax = plt.subplots(figsize=(8, 6))\n",
    "    sns.heatmap(\n",
    "        cm_norm, annot=True, cmap=\"Blues\", square=True, xticklabels=class_names, yticklabels=class_names\n",
    "    )\n",
    "    plt.ylabel(\"True label\")\n",
    "    plt.xlabel(\"Predicted label\")\n",
    "    # Log the confusion matrix plot to wandb\n",
    "    wandb.log({\"Confusion Matrix\":wandb.Image(fig)})\n",
    "    # log = {'conf_matrix':wandb.plot.confusion_matrix(y_true=y_test,preds=y_test_pred,class_names=class_names)}\n",
    "    # wandb.log(log)\n",
    "    # wandb.log({'conf_mat_'+wandb.run.name:wandb.plot.confusion_matrix(y_true=y_test,preds=y_test_pred,class_names=class_names)})\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wandb.agent(sweep_id,learn,count=60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(input_data, diff=False):\n",
    "    if not diff:\n",
    "        output_data = 1 / (1 + np.exp(-np.array(input_data)))\n",
    "    else:\n",
    "        s = 1 / (1 + np.exp(-np.array(input_data)))\n",
    "        output_data = s * (1 - s)\n",
    "    return output_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\HP\\AppData\\Local\\Temp\\ipykernel_3892\\937238283.py:3: RuntimeWarning: overflow encountered in exp\n",
      "  output_data = 1 / (1 + np.exp(-np.array(input_data)))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([1.84036862e-244, 0.00000000e+000, 1.76203358e-272, 0.00000000e+000,\n",
       "       1.97979520e-006, 0.00000000e+000, 2.14285538e-218, 5.90386593e-105,\n",
       "       3.86717302e-065, 7.16624766e-190])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = -1000*np.random.rand(10)\n",
    "sigmoid(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([-561.2207968 , -777.5664387 , -625.73667671, -949.93111217,\n",
       "         -13.13251517, -872.82178577, -501.20141104, -239.99582739,\n",
       "        -148.31550729, -435.52178549]),\n",
       " array([-561.2207968 , -700.        , -625.73667671, -700.        ,\n",
       "         -13.13251517, -700.        , -501.20141104, -239.99582739,\n",
       "        -148.31550729, -435.52178549]))"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
